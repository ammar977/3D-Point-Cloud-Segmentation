{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from itertools import combinations_with_replacement,product\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "range_min = 0\n",
    "range_max = 9\n",
    "batch_size = 4 \n",
    "num_classes = 82\n",
    "num_train_samples = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(list(product(range(10),repeat = 2)))\n",
    "np.random.shuffle(x)\n",
    "y = x[:,0] * x[:,1]\n",
    "train_labels = torch.from_numpy(y).type(torch.LongTensor)\n",
    "train_data = torch.from_numpy(x).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.fc1 = nn.Linear(2,30)\n",
    "        self.fc2 = nn.Linear(30,82)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.log_softmax(self.fc2(x))\n",
    "        return x \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CCELoss(preds,labels) :\n",
    "    logsoftmax = nn.LogSoftmax()\n",
    "    return torch.mean(torch.sum(-labels * logsoftmax(preds), dim=1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ammar\\.conda\\envs\\neuralnets\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  0    loss :  132.4525911808014\n",
      "epoch :  1    loss :  72.27609372138977\n",
      "epoch :  2    loss :  40.0239999294281\n",
      "epoch :  3    loss :  26.60797655582428\n",
      "epoch :  4    loss :  21.195862412452698\n",
      "epoch :  5    loss :  18.270841658115387\n",
      "epoch :  6    loss :  16.478437900543213\n",
      "epoch :  7    loss :  15.217865705490112\n",
      "epoch :  8    loss :  14.224665403366089\n",
      "epoch :  9    loss :  13.408781111240387\n",
      "epoch :  10    loss :  12.726604491472244\n",
      "epoch :  11    loss :  12.170313209295273\n",
      "epoch :  12    loss :  11.714036494493484\n",
      "epoch :  13    loss :  11.328298181295395\n",
      "epoch :  14    loss :  11.000174462795258\n",
      "epoch :  15    loss :  10.72037398815155\n",
      "epoch :  16    loss :  10.4792340695858\n",
      "epoch :  17    loss :  10.26956456899643\n",
      "epoch :  18    loss :  10.085381478071213\n",
      "epoch :  19    loss :  9.922606945037842\n",
      "epoch :  20    loss :  9.778224915266037\n",
      "epoch :  21    loss :  9.64798504114151\n",
      "epoch :  22    loss :  9.531020790338516\n",
      "epoch :  23    loss :  9.424291908740997\n",
      "epoch :  24    loss :  9.326148837804794\n",
      "epoch :  25    loss :  9.236646980047226\n",
      "epoch :  26    loss :  9.152811348438263\n",
      "epoch :  27    loss :  9.074328690767288\n",
      "epoch :  28    loss :  9.00060921907425\n",
      "epoch :  29    loss :  8.931545227766037\n",
      "epoch :  30    loss :  8.865555733442307\n",
      "epoch :  31    loss :  8.802098631858826\n",
      "epoch :  32    loss :  8.741075545549393\n",
      "epoch :  33    loss :  8.682156175374985\n",
      "epoch :  34    loss :  8.624926000833511\n",
      "epoch :  35    loss :  8.569086641073227\n",
      "epoch :  36    loss :  8.514356791973114\n",
      "epoch :  37    loss :  8.460551351308823\n",
      "epoch :  38    loss :  8.408156096935272\n",
      "epoch :  39    loss :  8.355119377374649\n",
      "epoch :  40    loss :  8.299280941486359\n",
      "epoch :  41    loss :  8.245098233222961\n",
      "epoch :  42    loss :  8.192851096391678\n",
      "epoch :  43    loss :  8.141353756189346\n",
      "epoch :  44    loss :  8.089747428894043\n",
      "epoch :  45    loss :  8.037726253271103\n",
      "epoch :  46    loss :  7.985140919685364\n",
      "epoch :  47    loss :  7.931813418865204\n",
      "epoch :  48    loss :  7.877648055553436\n",
      "epoch :  49    loss :  7.822551608085632\n",
      "epoch :  50    loss :  7.766377121210098\n",
      "epoch :  51    loss :  7.7090834975242615\n",
      "epoch :  52    loss :  7.650509357452393\n",
      "epoch :  53    loss :  7.590546250343323\n",
      "epoch :  54    loss :  7.529135882854462\n",
      "epoch :  55    loss :  7.466227740049362\n",
      "epoch :  56    loss :  7.401657998561859\n",
      "epoch :  57    loss :  7.335367530584335\n",
      "epoch :  58    loss :  7.267277330160141\n",
      "epoch :  59    loss :  7.197332322597504\n",
      "epoch :  60    loss :  7.12544447183609\n",
      "epoch :  61    loss :  7.051581293344498\n",
      "epoch :  62    loss :  6.975651890039444\n",
      "epoch :  63    loss :  6.897654622793198\n",
      "epoch :  64    loss :  6.817526996135712\n",
      "epoch :  65    loss :  6.735224217176437\n",
      "epoch :  66    loss :  6.650734305381775\n",
      "epoch :  67    loss :  6.564063459634781\n",
      "epoch :  68    loss :  6.475181400775909\n",
      "epoch :  69    loss :  6.383791297674179\n",
      "epoch :  70    loss :  6.289200723171234\n",
      "epoch :  71    loss :  6.192382022738457\n",
      "epoch :  72    loss :  6.093549996614456\n",
      "epoch :  73    loss :  5.99277625977993\n",
      "epoch :  74    loss :  5.8901414424180984\n",
      "epoch :  75    loss :  5.785710573196411\n",
      "epoch :  76    loss :  5.679614871740341\n",
      "epoch :  77    loss :  5.571929305791855\n",
      "epoch :  78    loss :  5.462776139378548\n",
      "epoch :  79    loss :  5.352302461862564\n",
      "epoch :  80    loss :  5.240647107362747\n",
      "epoch :  81    loss :  5.127982318401337\n",
      "epoch :  82    loss :  5.014462500810623\n",
      "epoch :  83    loss :  4.900314822793007\n",
      "epoch :  84    loss :  4.785697311162949\n",
      "epoch :  85    loss :  4.670710936188698\n",
      "epoch :  86    loss :  4.555614918470383\n",
      "epoch :  87    loss :  4.440594747662544\n",
      "epoch :  88    loss :  4.325852081179619\n",
      "epoch :  89    loss :  4.2115707993507385\n",
      "epoch :  90    loss :  4.097940057516098\n",
      "epoch :  91    loss :  3.9851502031087875\n",
      "epoch :  92    loss :  3.8733800053596497\n",
      "epoch :  93    loss :  3.7628528624773026\n",
      "epoch :  94    loss :  3.6537898182868958\n",
      "epoch :  95    loss :  3.5460774153470993\n",
      "epoch :  96    loss :  3.4400312155485153\n",
      "epoch :  97    loss :  3.3357733637094498\n",
      "epoch :  98    loss :  3.233417958021164\n",
      "epoch :  99    loss :  3.133074715733528\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0 \n",
    "    batch_min = 0\n",
    "    batch_max = batch_size\n",
    "    for batch in range(num_train_samples // batch_size):\n",
    "        data = train_data[batch_min:batch_min+batch_size]\n",
    "        labels = train_labels[batch_min:batch_min+batch_size]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out = net.forward(data)\n",
    "        loss = criterion(out,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss+=loss.item()\n",
    "    \n",
    "    batch_min += batch_size\n",
    "    batch_max += batch_size\n",
    "    print(\"epoch : \",epoch, \"   loss : \",running_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ammar\\.conda\\envs\\neuralnets\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45  5 64 81 45 64  5 64 64 64 64  5 64  5 64  5 64 64 64 64 64  5  5 64\n",
      " 64  5 64 45 64 64 64  5 64 64 64  5 64  5 64 64  5 64 64 64 64 64 64 64\n",
      "  5 64  5 64  5 64 64 64 64  5 45  5 64 64 64  5  5 64 64 64 64 81 64 64\n",
      " 64 64 45 64  5 64 64 45 64 64 64 64 64 64 64 64 64 45 64 64 64 64 64 45\n",
      " 64 45  5 64]\n"
     ]
    }
   ],
   "source": [
    "pred = net.forward(train_data).exp().detach()\n",
    "_,indices = torch.max(pred,1)\n",
    "indices = indices.numpy()\n",
    "print(indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9., 5.],\n",
       "        [5., 1.],\n",
       "        [8., 8.],\n",
       "        [9., 9.],\n",
       "        [8., 5.],\n",
       "        [8., 7.],\n",
       "        [6., 2.],\n",
       "        [7., 7.],\n",
       "        [6., 7.],\n",
       "        [3., 9.],\n",
       "        [6., 5.],\n",
       "        [7., 0.],\n",
       "        [4., 3.],\n",
       "        [9., 3.],\n",
       "        [5., 9.],\n",
       "        [6., 1.],\n",
       "        [3., 6.],\n",
       "        [1., 2.],\n",
       "        [2., 4.],\n",
       "        [0., 0.],\n",
       "        [4., 8.],\n",
       "        [3., 0.],\n",
       "        [8., 3.],\n",
       "        [2., 1.],\n",
       "        [3., 2.],\n",
       "        [8., 1.],\n",
       "        [7., 6.],\n",
       "        [9., 4.],\n",
       "        [0., 7.],\n",
       "        [1., 0.],\n",
       "        [4., 4.],\n",
       "        [9., 0.],\n",
       "        [3., 4.],\n",
       "        [0., 8.],\n",
       "        [5., 5.],\n",
       "        [9., 1.],\n",
       "        [6., 9.],\n",
       "        [9., 2.],\n",
       "        [6., 4.],\n",
       "        [6., 8.],\n",
       "        [4., 1.],\n",
       "        [1., 3.],\n",
       "        [5., 7.],\n",
       "        [7., 8.],\n",
       "        [3., 3.],\n",
       "        [1., 6.],\n",
       "        [0., 2.],\n",
       "        [1., 7.],\n",
       "        [8., 0.],\n",
       "        [1., 4.],\n",
       "        [5., 0.],\n",
       "        [0., 3.],\n",
       "        [5., 2.],\n",
       "        [2., 3.],\n",
       "        [7., 9.],\n",
       "        [2., 5.],\n",
       "        [9., 8.],\n",
       "        [7., 1.],\n",
       "        [8., 4.],\n",
       "        [6., 0.],\n",
       "        [0., 5.],\n",
       "        [8., 6.],\n",
       "        [2., 6.],\n",
       "        [8., 2.],\n",
       "        [2., 0.],\n",
       "        [1., 8.],\n",
       "        [1., 5.],\n",
       "        [5., 3.],\n",
       "        [3., 5.],\n",
       "        [8., 9.],\n",
       "        [6., 6.],\n",
       "        [4., 2.],\n",
       "        [2., 8.],\n",
       "        [0., 4.],\n",
       "        [7., 4.],\n",
       "        [0., 9.],\n",
       "        [4., 0.],\n",
       "        [2., 7.],\n",
       "        [5., 8.],\n",
       "        [9., 7.],\n",
       "        [5., 6.],\n",
       "        [7., 5.],\n",
       "        [1., 1.],\n",
       "        [4., 7.],\n",
       "        [5., 4.],\n",
       "        [3., 8.],\n",
       "        [3., 7.],\n",
       "        [2., 9.],\n",
       "        [3., 1.],\n",
       "        [9., 6.],\n",
       "        [4., 6.],\n",
       "        [1., 9.],\n",
       "        [0., 1.],\n",
       "        [2., 2.],\n",
       "        [4., 5.],\n",
       "        [7., 3.],\n",
       "        [4., 9.],\n",
       "        [6., 3.],\n",
       "        [7., 2.],\n",
       "        [0., 6.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
